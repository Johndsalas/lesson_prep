{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accepting-example",
   "metadata": {},
   "source": [
    "# Acquire Zillow\n",
    "\n",
    "## For the following, iterate through the steps you would take to create functions: Write the code to do the following in a jupyter notebook, test it, convert to functions, then create the file to house those functions.\n",
    "\n",
    "## You will have a zillow.ipynb file and a helper file for each section in the pipeline.\n",
    "\n",
    "* Acquire data from mySQL using the python module to connect and query. You will want to end with a single dataframe.\n",
    "\n",
    "* Make sure to include: the logerror, all fields related to the properties that are available. You will end up using all the tables in the database.\n",
    "\n",
    "* Be sure to do the correct join (inner, outer, etc.). We do not want to eliminate properties purely because they may have a null value for airconditioningtypeid.\n",
    "\n",
    "* Only include properties with a transaction in 2017, and include only the last transaction for each property (so no duplicate property ID's), along with zestimate error and date of transaction.\n",
    "* Only include properties that include a latitude and longitude value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-swiss",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from env import user, host, password\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-drove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_connection(database, user=user, host=host, password=password):\n",
    "    '''get URL with user, host, and password from env '''\n",
    "    \n",
    "    return f\"mysql+pymysql://{user}:{password}@{host}/{database}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-alignment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sql_data(database,query):\n",
    "    ''' \n",
    "        Take in a database and query\n",
    "        check if csv exists for the queried database\n",
    "        if it does read from the csv\n",
    "        if it does not create the csv then read from the csv  \n",
    "    '''\n",
    "    \n",
    "    if os.path.isfile(f'{database}_query.csv') == False:   # check for the file\n",
    "        \n",
    "        df = pd.read_sql(query, get_connection(database))  # create df for query\n",
    "        \n",
    "        df.to_csv(f'{database}_query.csv',index = False)   # cache file\n",
    "        \n",
    "    return pd.read_csv(f'{database}_query.csv') # return contents of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-integral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "\n",
    "select * \n",
    "from predictions_2017\n",
    "\n",
    "left join properties_2017 using(parcelid)\n",
    "left join airconditioningtype using(airconditioningtypeid)\n",
    "left join architecturalstyletype using(architecturalstyletypeid)\n",
    "left join buildingclasstype using(buildingclasstypeid)\n",
    "left join heatingorsystemtype using(heatingorsystemtypeid)\n",
    "left join propertylandusetype using(propertylandusetypeid)\n",
    "left join storytype using(storytypeid)\n",
    "left join typeconstructiontype using(typeconstructiontypeid)\n",
    "\n",
    "where latitude is not null\n",
    "\n",
    "and longitude is not null\n",
    "\n",
    "'''\n",
    "\n",
    "database = \"zillow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-diana",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_sql_data(database,query)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-efficiency",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-creativity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop duplicate parcelid keeping the latest one by transaction date\n",
    "df = df.sort_values('transactiondate').drop_duplicates('parcelid',keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-mozambique",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-edition",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.parcelid.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-knowing",
   "metadata": {},
   "source": [
    "# Summarize your data (summary stats, info, dtypes, shape, distributions, value_counts, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-running",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-player",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-meaning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get value counts for discrete variables\n",
    "\n",
    "desc_cols = [col for col in df.columns if (df[col].dtype == \"object\")]\n",
    "\n",
    "\n",
    "for col in desc_cols:\n",
    "    \n",
    "    print(col)\n",
    "    print(df[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-source",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of the data\n",
    "con_cols = [col for col in df.columns if (df[col].dtype == 'int64') | (df[col].dtype == 'float64')]\n",
    "\n",
    "for col in con_cols:\n",
    "    plt.hist(df[col])\n",
    "    plt.title(f\"{col} distribution\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-regulation",
   "metadata": {},
   "source": [
    "# Write a function that takes in a dataframe of observations and attributes and returns a dataframe where each row is an atttribute name, the first column is the number of rows with missing values for that attribute, and the second column is percent of total rows that have missing values for that attribute. Run the function and document takeaways from this on how you want to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # get columns paired with the number of nulls in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()/df.shape[0] # get percent of nulls in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing values in variables\n",
    "\n",
    "num_missing = df.isnull().sum()\n",
    "pct_missing = df.isnull().sum()/df.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'num_rows_missing': num_missing, 'pct_rows_missing': pct_missing}) # create dataframe using variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_by_col(df):\n",
    "    '''\n",
    "    take in a dataframe \n",
    "    return a dataframe with each cloumn name as a row \n",
    "    each row will show the number and percent of nulls in the column\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    num_missing = df.isnull().sum()   # get columns paired with the number of nulls in that column\n",
    "    \n",
    "    pct_missing = df.isnull().sum()/df.shape[0] # get percent of nulls in each column\n",
    "    \n",
    "    return pd.DataFrame({'num_rows_missing': num_missing, 'pct_rows_missing': pct_missing}) # create/return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_by_col(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-piece",
   "metadata": {},
   "source": [
    "### Takeaways: \n",
    "* Columns have a largely varied number of nulls \n",
    "* Drop columns with less than half non_null values\n",
    "* Treat the rest as a case by case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-north",
   "metadata": {},
   "source": [
    "# Write a function that takes in a dataframe and returns a dataframe with 3 columns: the number of columns missing, percent of columns missing, and number of rows with n columns missing. Run the function and document takeaways from this on how you want to handle missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1) # number of columns that are missing in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=1)/df.shape[1]*100 # percent of columns missing in each row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture series in veriables\n",
    "\n",
    "num_cols_missing = df.isnull().sum(axis=1)\n",
    "\n",
    "pct_cols_missing = df.isnull().sum(axis=1)/df.shape[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for the series and reset the index creating an index column\n",
    "\n",
    "df_cols = pd.DataFrame({'num_cols_missing': num_cols_missing, 'pct_cols_missing': pct_cols_missing}).reset_index()\n",
    "df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by count of both columns, turns index column into a count of matching rows\n",
    "\n",
    "df_cols = df_cols.groupby(['num_cols_missing','pct_cols_missing']).count()\n",
    "df_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index name and reset the index\n",
    "\n",
    "df_cols = df_cols.rename(index=str, columns={'index': 'num_rows'}).reset_index()\n",
    "df_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nulls_by_row(df):\n",
    "    '''take in a dataframe \n",
    "       get count of missing columns per row\n",
    "       percent of missing columns per row \n",
    "       and number of rows missing the same number of columns\n",
    "       in a dataframe'''\n",
    "    \n",
    "    num_cols_missing = df.isnull().sum(axis=1) # number of columns that are missing in each row\n",
    "    \n",
    "    pct_cols_missing = df.isnull().sum(axis=1)/df.shape[1]*100  # percent of columns missing in each row \n",
    "    \n",
    "    # create a dataframe for the series and reset the index creating an index column\n",
    "    # group by count of both columns, turns index column into a count of matching rows\n",
    "    # change the index name and reset the index\n",
    "    \n",
    "    return (pd.DataFrame({'num_cols_missing': num_cols_missing, 'pct_cols_missing': pct_cols_missing}).reset_index()\n",
    "            .groupby(['num_cols_missing','pct_cols_missing']).count()\n",
    "            .rename(index=str, columns={'index': 'num_rows'}).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls_by_row(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-sample",
   "metadata": {},
   "source": [
    "### Takeaways: \n",
    "* Rows missing between 33.33 and 70 percent of cols\n",
    "* Drop rows with less than half non_null cols\n",
    "* Treat the rest as a case by case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-poster",
   "metadata": {},
   "source": [
    "# Prepare\n",
    "\n",
    "## Remove any properties that are likely to be something other than single unit properties. (e.g. no duplexes, no land/lot, ...). There are multiple ways to estimate that a property is a single unit, and there is not a single \"right\" answer. But for this exercise, do not purely filter by unitcnt as we did previously. Add some new logic that will reduce the number of properties that are falsely removed. You might want to use # bedrooms, square feet, unit type or the like to then identify those with unitcnt not defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.propertylandusedesc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows based on propertylanduse\n",
    "df = df[(df.propertylandusedesc == 'Single Family Residential') |\n",
    "          (df.propertylandusedesc == 'Mobile Home') |\n",
    "          (df.propertylandusedesc == 'Manufactured, Modular, Prefabricated Homes') |\n",
    "          (df.propertylandusedesc == 'Townhouse')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median area 2017: 2,400\n",
    "\n",
    "df.boxplot(column='calculatedfinishedsquarefeet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='bedroomcnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='bathroomcnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[f'{col}'].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        df = df[(df[f'{col}'] > lower_bound) & (df[f'{col}'] < upper_bound)]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_outliers(df, 1.5, ['calculatedfinishedsquarefeet', 'bedroomcnt', 'bathroomcnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-paris",
   "metadata": {},
   "source": [
    "# Create a function that will drop rows or columns based on the percent of values that are missing: handle_missing_values(df, prop_required_column, prop_required_row).\n",
    "\n",
    "## The input:\n",
    "* A dataframe\n",
    "* A number between 0 and 1 that represents the proportion, for each column, of rows with non-missing values required to keep the column. i.e. if prop_required_column = .6, then you are requiring a column to have at least 60% of values not-NA (no more than 40% missing).\n",
    "* A number between 0 and 1 that represents the proportion, for each row, of columns/variables with non-missing values required to keep the row. For example, if prop_required_row = .75, then you are requiring a row to have at least 75% of variables with a non-missing value (no more that 25% missing).\n",
    "\n",
    "## The output:\n",
    "* The dataframe with the columns and rows dropped as indicated. Be sure to drop the columns prior to the rows in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test proportions\n",
    "prop_required_column = .5\n",
    "prop_required_row = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(round(prop_required_column*df.shape[0],0)) # get minimum acceptable nulls in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set as thresh\n",
    "\n",
    "col_thresh = int(round(prop_required_column*df.shape[0],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns based on threshold number\n",
    "df.dropna(axis=1, thresh=col_thresh, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for rows \n",
    "\n",
    "row_thresh = int(round(prop_required_row*df.shape[1],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, thresh=row_thresh, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, prop_required_column = .5, prop_required_row = .5):\n",
    "    ''' \n",
    "        take in a dataframe and a proportion for columns and rows\n",
    "        return dataframe with columns and rows not meeting proportions dropped\n",
    "    '''\n",
    "    col_thresh = int(round(prop_required_column*df.shape[0],0)) # calc column threshold\n",
    "    \n",
    "    df.dropna(axis=1, thresh=col_thresh, inplace=True) # drop columns with non-nulls less than threshold\n",
    "    \n",
    "    row_thresh = int(round(prop_required_row*df.shape[1],0))  # calc row threshhold\n",
    "    \n",
    "    df.dropna(axis=0, thresh=row_thresh, inplace=True) # drop columns with non-nulls less than threshold\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-cattle",
   "metadata": {},
   "source": [
    "# Decide how to handle the remaining missing values:\n",
    "\n",
    "* Fill with constant value.\n",
    "* Impute with mean, median, mode.\n",
    "* Drop row/column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-blowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum()>0] # look at cols with nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum()>16000] # look at cols with large amounts of nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for cals with large amounts of nulls\n",
    "for col in ['heatingorsystemtypeid', 'buildingqualitytypeid', 'propertyzoningdesc', 'unitcnt', 'heatingorsystemdesc']:\n",
    "    \n",
    "    print(col)\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-portal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns with 16K missing values too much to fill/impute/drop rows\n",
    "\n",
    "df = df.drop(columns=['heatingorsystemtypeid', 'buildingqualitytypeid', 'propertyzoningdesc', 'unitcnt', 'heatingorsystemdesc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum()>0] # look at remaining nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value counts for remaining cols that are descreet\n",
    "\n",
    "for col in ['calculatedbathnbr', 'fullbathcnt', 'regionidcity', 'regionidzip', 'yearbuilt', 'censustractandblock']:\n",
    "    \n",
    "    print(col)\n",
    "    print(df[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute columns *do this after you split*\n",
    "\n",
    "def impute(df, my_strategy, column_list):\n",
    "    ''' take in a df strategy and cloumn list\n",
    "        return df with listed columns imputed using input stratagy\n",
    "    '''\n",
    "        \n",
    "    imputer = SimpleImputer(strategy=my_strategy)  # build imputer\n",
    "\n",
    "    df[column_list] = imputer.fit_transform(df[column_list]) # fit/transform selected columns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute(df, 'most_frequent', ['calculatedbathnbr', 'fullbathcnt', 'regionidcity', 'regionidzip', 'yearbuilt', 'censustractandblock'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum()>0] # look at remaining nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = impute(df, 'median', ['finishedsquarefeet12', 'lotsizesquarefeet', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-parking",
   "metadata": {},
   "source": [
    "# wrangle_zillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############acquire#################\n",
    "\n",
    "\n",
    "def get_connection(database, user=user, host=host, password=password):\n",
    "    '''get URL with user, host, and password from env '''\n",
    "    \n",
    "    return f\"mysql+pymysql://{user}:{password}@{host}/{database}\"\n",
    "    \n",
    "    \n",
    "def cache_sql_data(df, database):\n",
    "    '''write dataframe to csv with title database_query.csv'''\n",
    "    \n",
    "    df.to_csv(f'{database}_query.csv',index = False)\n",
    "        \n",
    "\n",
    "def get_sql_data(database,query):\n",
    "    ''' check if csv exists for the queried database\n",
    "        if it does read from the csv\n",
    "        if it does not create the csv then read from the csv  \n",
    "    '''\n",
    "    \n",
    "    if os.path.isfile(f'{database}_query.csv') == False:   # check for the file\n",
    "        \n",
    "        df = pd.read_sql(query, get_connection(database))  # create file \n",
    "        \n",
    "        cache_sql_data(df, database) # cache file\n",
    "        \n",
    "    return pd.read_csv(f'{database}_query.csv') # return contents of file\n",
    "\n",
    "\n",
    "def get_zillow_data():\n",
    "    ''' acquire zillow data'''\n",
    "    \n",
    "    query = '''\n",
    "\n",
    "    select * \n",
    "    from predictions_2017\n",
    "\n",
    "    left join properties_2017 using(parcelid)\n",
    "    left join airconditioningtype using(airconditioningtypeid)\n",
    "    left join architecturalstyletype using(architecturalstyletypeid)\n",
    "    left join buildingclasstype using(buildingclasstypeid)\n",
    "    left join heatingorsystemtype using(heatingorsystemtypeid)\n",
    "    left join propertylandusetype using(propertylandusetypeid)\n",
    "    left join storytype using(storytypeid)\n",
    "    left join typeconstructiontype using(typeconstructiontypeid)\n",
    "\n",
    "    where latitude is not null\n",
    "\n",
    "    and longitude is not null\n",
    "\n",
    "    '''\n",
    "\n",
    "    database = \"zillow\"\n",
    "    \n",
    "    df = get_sql_data(database,query) # create/read csv for query\n",
    "    \n",
    "    df = df.sort_values('transactiondate').drop_duplicates('parcelid',keep='last') # drop duplicate parcelids keeping the latest\n",
    "    \n",
    "    return df \n",
    "\n",
    "#################################prepare###############################\n",
    "\n",
    "def remove_outliers(df, k, col_list):\n",
    "    ''' remove outliers from a list of columns in a dataframe \n",
    "        and return that dataframe\n",
    "    '''\n",
    "    \n",
    "    for col in col_list:\n",
    "\n",
    "        q1, q3 = df[f'{col}'].quantile([.25, .75])  # get quartiles\n",
    "        \n",
    "        iqr = q3 - q1   # calculate interquartile range\n",
    "        \n",
    "        upper_bound = q3 + k * iqr   # get upper bound\n",
    "        lower_bound = q1 - k * iqr   # get lower bound\n",
    "\n",
    "        # return dataframe without outliers\n",
    "        \n",
    "        return df[(df[f'{col}'] > lower_bound) & (df[f'{col}'] < upper_bound)]  \n",
    "    \n",
    "\n",
    "def handle_missing_values(df, prop_required_column = .5, prop_required_row = .5):\n",
    "    ''' \n",
    "        take in a dataframe and a proportion for columns and rows\n",
    "        return dataframe with columns and rows not meeting proportions dropped\n",
    "    '''\n",
    "    col_thresh = int(round(prop_required_column*df.shape[0],0)) # calc column threshold\n",
    "    \n",
    "    df.dropna(axis=1, thresh=col_thresh, inplace=True) # drop columns with non-nulls less than threshold\n",
    "    \n",
    "    row_thresh = int(round(prop_required_row*df.shape[1],0))  # calc row threshhold\n",
    "    \n",
    "    df.dropna(axis=0, thresh=row_thresh, inplace=True) # drop columns with non-nulls less than threshold\n",
    "    \n",
    "    return df    \n",
    "    \n",
    "    \n",
    "def impute(df, my_strategy, column_list):\n",
    "    ''' take in a df strategy and cloumn list\n",
    "        return df with listed columns imputed using input stratagy\n",
    "    '''\n",
    "        \n",
    "    imputer = SimpleImputer(strategy=my_strategy)  # build imputer\n",
    "\n",
    "    df[column_list] = imputer.fit_transform(df[column_list]) # fit/transform selected columns\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_zillow(df):\n",
    "    ''' Prepare Zillow Data'''\n",
    "    \n",
    "    # Restrict propertylandusedesc to those of single unit\n",
    "    df = df[(df.propertylandusedesc == 'Single Family Residential') |\n",
    "          (df.propertylandusedesc == 'Mobile Home') |\n",
    "          (df.propertylandusedesc == 'Manufactured, Modular, Prefabricated Homes') |\n",
    "          (df.propertylandusedesc == 'Townhouse')]\n",
    "    \n",
    "    # remove outliers in bed count, bath count, and area to better target single unit properties\n",
    "    df = remove_outliers(df, 1.5, ['calculatedfinishedsquarefeet', 'bedroomcnt', 'bathroomcnt'])\n",
    "    \n",
    "    # dropping cols/rows where more than half of the values are null\n",
    "    df = handle_missing_values(df, prop_required_column = .5, prop_required_row = .5)\n",
    "    \n",
    "    # dropping the columns with 17K missing values too much to fill/impute/drop rows\n",
    "    df = df.drop(columns=['heatingorsystemtypeid', 'buildingqualitytypeid', 'propertyzoningdesc', 'unitcnt', 'heatingorsystemdesc'])\n",
    "    \n",
    "    # imputing descreet columns with most frequent value\n",
    "    df = impute(df, 'most_frequent', ['calculatedbathnbr', 'fullbathcnt', 'regionidcity', 'regionidzip', 'yearbuilt', 'censustractandblock'])\n",
    "    \n",
    "    # imputing continuous columns with median value\n",
    "    df = impute(df, 'median', ['finishedsquarefeet12', 'lotsizesquarefeet', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxamount'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_zillow(get_zillow_data())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-toner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
